{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","metadata":{"id":"uV8Lch-wnW32","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1680795592814,"user_tz":240,"elapsed":2298,"user":{"displayName":"Pranath Reddy Kumbam","userId":"07194914569693395860"}},"outputId":"a3ccfc4a-0442-45a5-b2e1-4e6aeb6780d2"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","metadata":{"id":"Fltk46feoNyW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1680795592815,"user_tz":240,"elapsed":8,"user":{"displayName":"Pranath Reddy Kumbam","userId":"07194914569693395860"}},"outputId":"423d1164-a8a2-4997-b23f-91de24cba13f"},"source":["cd drive"],"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["[Errno 2] No such file or directory: 'drive'\n","/content/drive/My Drive/NLP\n"]}]},{"cell_type":"code","metadata":{"id":"22rFozDqoQxm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1680795592815,"user_tz":240,"elapsed":6,"user":{"displayName":"Pranath Reddy Kumbam","userId":"07194914569693395860"}},"outputId":"40289e1c-a021-401d-b1a0-1c3c89d30def"},"source":["cd My \\Drive"],"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["[Errno 2] No such file or directory: 'My Drive'\n","/content/drive/My Drive/NLP\n"]}]},{"cell_type":"code","metadata":{"id":"DuSmOjVKoU-S","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1680795592815,"user_tz":240,"elapsed":5,"user":{"displayName":"Pranath Reddy Kumbam","userId":"07194914569693395860"}},"outputId":"5b12ba38-c4c5-41fe-dc82-e458a9564772"},"source":["cd NLP/"],"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["[Errno 2] No such file or directory: 'NLP/'\n","/content/drive/My Drive/NLP\n"]}]},{"cell_type":"code","source":["# Load libraries\n","import pandas as pd\n","import nltk\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n","import numpy as np\n","from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import precision_recall_fscore_support\n","nltk.download('omw-1.4')\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","nltk.download('stopwords')\n","\n","# Load the dataset\n","df = pd.read_csv('./Data/KaggleData.csv')\n","\n","# Convert to lowercase, remove punctuation, extra spaces, URLs, mentions, and hashtags\n","df['tweet'] = df['tweet'].str.lower().replace(r'[^\\w\\s]', '', regex=True).replace(' {2,}', ' ', regex=True).replace('\"', '')\n","df['tweet'] = df['tweet'].replace(r'http\\S+|www.\\S+|@\\w+|#\\w+', '', regex=True)\n","\n","# Tokenization\n","nltk.download('punkt')\n","df['tweet'] = df['tweet'].apply(nltk.word_tokenize)\n","\n","# Lemmatization\n","nltk.download('wordnet')\n","lemmatizer = WordNetLemmatizer()\n","df['tweet'] = df['tweet'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n","\n","# Removing stop-words\n","nltk.download('stopwords')\n","stop_words = set(stopwords.words('english'))\n","df['tweet'] = df['tweet'].apply(lambda x: ' '.join([word for word in x if word not in stop_words]))\n","\n","# Create a custom dataset class\n","class TextDataset(Dataset):\n","    def __init__(self, texts, labels):\n","        self.texts = texts\n","        self.labels = labels\n","\n","    def __len__(self):\n","        return len(self.texts)\n","\n","    def __getitem__(self, idx):\n","        return self.texts[idx], self.labels[idx]\n","\n","# Encode the labels\n","# 0 - hate speech, 1 - offensive language, 2 - neither as positive or negative\n","label_encoder = LabelEncoder()\n","y = label_encoder.fit_transform(df['class'])\n","\n","# Splitting the Data using Stratified split\n","X_train, X_test, y_train, y_test = train_test_split(df['tweet'], y, test_size=0.3, stratify=y, random_state=42)\n","\n","# Tokenize and pad the input sequences\n","def tokenize_and_pad(texts, maxlen=100):\n","    tokenized_texts = [nltk.word_tokenize(text) for text in texts]\n","    return pad_sequence([torch.tensor([word_to_index[word] for word in text if word in word_to_index][:maxlen]) for text in tokenized_texts], batch_first=True, padding_value=len(word_to_index))\n","\n","word_to_index = {word: i for i, word in enumerate(set(df['tweet'].str.cat(sep=' ').split()), 1)}\n","X_train = tokenize_and_pad(X_train)\n","X_test = tokenize_and_pad(X_test)\n","\n","# Create PyTorch Datasets and DataLoaders\n","train_dataset = TextDataset(X_train, y_train)\n","test_dataset = TextDataset(X_test, y_test)\n","train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n","test_loader = DataLoader(test_dataset, batch_size=32)\n","\n","# Create a PyTorch LSTM model\n","class LSTMBaseline(nn.Module):\n","    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_classes):\n","        super().__init__()\n","        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n","        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n","        self.fc = nn.Linear(hidden_dim, num_classes)\n","\n","    def forward(self, x):\n","        x = self.embedding(x)\n","        packed_output, (hidden, cell) = self.lstm(x)\n","        x = self.fc(hidden[-1])\n","        return x\n","\n","# Initialize the model, optimizer, and loss function\n","model = LSTMBaseline(len(word_to_index) + 1, 50, 100, len(set(y)))\n","optimizer = optim.Adam(model.parameters(), lr=1e-3)\n","criterion = nn.CrossEntropyLoss()\n","    \n","# Train the model\n","epochs = 10\n","for epoch in range(epochs):\n","    model.train()\n","    epoch_loss = 0\n","\n","    for texts, labels in train_loader:\n","        optimizer.zero_grad()\n","        outputs = model(texts)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        epoch_loss += loss.item()\n","\n","    print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss / len(train_loader)}\")\n","\n","    # Save Model\n","    torch.save(model, './Weights/KaggleLSTM.pth')\n","\n","# Test the model and collect predictions and true labels\n","model.eval()\n","predictions = []\n","true_labels = []\n","\n","with torch.no_grad():\n","    for texts, labels in test_loader:\n","        outputs = model(texts)\n","        _, predicted = torch.max(outputs, 1)\n","        predictions.extend(predicted.numpy())\n","        true_labels.extend(labels.numpy())\n","\n","# Calculate accuracy, precision, recall, F1-score, and confusion matrix\n","accuracy = np.mean(np.array(predictions) == np.array(true_labels))\n","precision, recall, f1_score, _ = precision_recall_fscore_support(true_labels, predictions, average='weighted')\n","conf_mat = confusion_matrix(true_labels, predictions)\n","\n","print(\"Accuracy: \", accuracy)\n","print(\"Precision: \", precision)\n","print(\"Recall: \", recall)\n","print(\"F1-score: \", f1_score)\n","print(\"Confusion Matrix:\\n\", conf_mat)"],"metadata":{"id":"ZMUG3iDxuEny","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1680795724707,"user_tz":240,"elapsed":131895,"user":{"displayName":"Pranath Reddy Kumbam","userId":"07194914569693395860"}},"outputId":"732d0cea-1cf5-48b3-fdf0-9d3eefa6f086"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n","[nltk_data]   Package omw-1.4 is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10, Loss: 0.6682774611160461\n","Epoch 2/10, Loss: 0.6648849582035458\n","Epoch 3/10, Loss: 0.4339797409391974\n","Epoch 4/10, Loss: 0.34845726736040844\n","Epoch 5/10, Loss: 0.30326250335458893\n","Epoch 6/10, Loss: 0.26482036016181687\n","Epoch 7/10, Loss: 0.2358285127341418\n","Epoch 8/10, Loss: 0.20508714285413315\n","Epoch 9/10, Loss: 0.18264081944500543\n","Epoch 10/10, Loss: 0.1611840561234682\n","Accuracy:  0.8664425016812374\n","Precision:  0.8530193150878194\n","Recall:  0.8664425016812374\n","F1-score:  0.8577859260533811\n","Confusion Matrix:\n"," [[  91  281   57]\n"," [ 100 5398  259]\n"," [  33  263  953]]\n"]}]}]}