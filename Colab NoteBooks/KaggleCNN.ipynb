{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","metadata":{"id":"uV8Lch-wnW32","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1680794587292,"user_tz":240,"elapsed":15853,"user":{"displayName":"Pranath Reddy Kumbam","userId":"07194914569693395860"}},"outputId":"bd4b1f4a-0299-49f7-921a-b0777ba9a5aa"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"id":"Fltk46feoNyW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1680794587292,"user_tz":240,"elapsed":8,"user":{"displayName":"Pranath Reddy Kumbam","userId":"07194914569693395860"}},"outputId":"28e3841b-68bf-4415-92b2-4d92e8640dfd"},"source":["cd drive"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive\n"]}]},{"cell_type":"code","metadata":{"id":"22rFozDqoQxm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1680794587293,"user_tz":240,"elapsed":5,"user":{"displayName":"Pranath Reddy Kumbam","userId":"07194914569693395860"}},"outputId":"cf339a05-9d4e-4018-95bc-4d6068b4d221"},"source":["cd My \\Drive"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/My Drive\n"]}]},{"cell_type":"code","metadata":{"id":"DuSmOjVKoU-S","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1680794587462,"user_tz":240,"elapsed":173,"user":{"displayName":"Pranath Reddy Kumbam","userId":"07194914569693395860"}},"outputId":"245958d4-b8f2-45c6-dfe4-5a70992a56b8"},"source":["cd NLP/"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/My Drive/NLP\n"]}]},{"cell_type":"code","source":["# Load libraries\n","import pandas as pd\n","import numpy as np\n","import nltk\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, Dataset, TensorDataset\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","from sklearn.metrics import accuracy_score\n","import torch.nn.functional as F\n","import numpy as np\n","from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import precision_recall_fscore_support\n","\n","# Load the dataset\n","df = pd.read_csv('./Data/KaggleData.csv')\n","\n","# Convert to lowercase, remove punctuation, extra spaces, URLs, mentions, and hashtags\n","df['tweet'] = df['tweet'].str.lower().replace(r'[^\\w\\s]', '', regex=True).replace(' {2,}', ' ', regex=True).replace('\"', '')\n","df['tweet'] = df['tweet'].replace(r'http\\S+|www.\\S+|@\\w+|#\\w+', '', regex=True)\n","\n","# Tokenization\n","nltk.download('punkt')\n","df['tweet'] = df['tweet'].apply(nltk.word_tokenize)\n","\n","# Lemmatization\n","nltk.download('wordnet')\n","lemmatizer = WordNetLemmatizer()\n","df['tweet'] = df['tweet'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n","\n","# Removing stop-words\n","nltk.download('stopwords')\n","stop_words = set(stopwords.words('english'))\n","df['tweet'] = df['tweet'].apply(lambda x: ' '.join([word for word in x if word not in stop_words]))\n","\n","# Prepare data for DataLoader\n","max_length = 50\n","tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n","df['tweet'] = df['tweet'].apply(lambda x: tokenizer.tokenize(x)[:max_length])\n","\n","# Create word2index dictionary\n","word2index = {}\n","for tweet in df['tweet']:\n","    for word in tweet:\n","        if word not in word2index:\n","            word2index[word] = len(word2index)\n","\n","# Convert words to indices and pad sequences\n","input_data = np.zeros((len(df['tweet']), max_length), dtype=int)\n","for i, tweet in enumerate(df['tweet']):\n","    for j, word in enumerate(tweet):\n","        input_data[i, j] = word2index[word]\n","\n","# Encode labels\n","# 0 - hate speech, 1 - offensive language, 2 - neither as positive or negative\n","le = LabelEncoder()\n","y = le.fit_transform(df['class'])\n","\n","# Split the data\n","X_train, X_test, y_train, y_test = train_test_split(input_data, y, test_size=0.3, stratify=y, random_state=42)\n","\n","# Create DataLoader objects\n","batch_size = 64\n","train_dataset = TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))\n","test_dataset = TensorDataset(torch.from_numpy(X_test), torch.from_numpy(y_test))\n","train_loader = DataLoader(train_dataset, batch_size=batch_size)\n","test_loader = DataLoader(test_dataset, batch_size=batch_size)\n","\n","# Create CNN model\n","class TextCNN(nn.Module):\n","    def __init__(self, vocab_size, embed_dim, num_classes):\n","        super(TextCNN, self).__init__()\n","        self.embedding = nn.Embedding(vocab_size, embed_dim)\n","        self.conv1 = nn.Conv2d(1, 100, (3, embed_dim))\n","        self.conv2 = nn.Conv2d(1, 100, (4, embed_dim))\n","        self.conv3 = nn.Conv2d(1, 100, (5, embed_dim))\n","        self.fc = nn.Linear(300, num_classes)\n","\n","    def forward(self, x):\n","        x = self.embedding(x).unsqueeze(1)\n","        x1 = F.relu(self.conv1(x)).squeeze(3)\n","        x2 = F.relu(self.conv2(x)).squeeze(3)\n","        x3 = F.relu(self.conv3(x)).squeeze(3)\n","        x1 = F.max_pool1d(x1, x1.size(2)).squeeze(2)\n","        x2 = F.max_pool1d(x2, x2.size(2)).squeeze(2)\n","        x3 = F.max_pool1d(x3, x3.size(2)).squeeze(2)\n","        x = torch.cat((x1, x2, x3), 1)\n","        x = self.fc(x)\n","        return x\n","    \n","# Training\n","device = torch.device(\"cuda\")\n","vocab_size = len(word2index)\n","embed_dim = 100\n","num_classes = len(np.unique(y))\n","model = TextCNN(vocab_size, embed_dim, num_classes).to(device)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","num_epochs = 10\n","for epoch in range(num_epochs):\n","    model.train()\n","    running_loss = 0.0\n","    for i, (inputs, labels) in enumerate(train_loader):\n","        inputs, labels = inputs.to(device), labels.to(device)\n","        optimizer.zero_grad()\n","        outputs = model(inputs.long())\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        running_loss += loss.item()\n","\n","    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {running_loss / len(train_loader)}\")\n","\n","    # Save Model\n","    torch.save(model, './Weights/KaggleCNN.pth')\n","\n","# Test the model and collect predictions and true labels\n","model.eval()\n","predictions = []\n","true_labels = []\n","\n","with torch.no_grad():\n","    for inputs, labels in test_loader:\n","        inputs, labels = inputs.to(device), labels.to(device)\n","        outputs = model(inputs.long())\n","        _, predicted = torch.max(outputs, 1)\n","        predictions.extend(predicted.cpu().numpy())\n","        true_labels.extend(labels.cpu().numpy())\n","\n","# Calculate accuracy, precision, recall, F1-score, and confusion matrix\n","accuracy = np.mean(np.array(predictions) == np.array(true_labels))\n","precision, recall, f1_score, _ = precision_recall_fscore_support(true_labels, predictions, average='weighted')\n","conf_mat = confusion_matrix(true_labels, predictions)\n","\n","print(\"Accuracy: \", accuracy)\n","print(\"Precision: \", precision)\n","print(\"Recall: \", recall)\n","print(\"F1-score: \", f1_score)\n","print(\"Confusion Matrix:\\n\", conf_mat)"],"metadata":{"id":"ZMUG3iDxuEny","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1680794750890,"user_tz":240,"elapsed":163430,"user":{"displayName":"Pranath Reddy Kumbam","userId":"07194914569693395860"}},"outputId":"01ffcac3-f070-4666-c168-e22f823e7c3f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10, Loss: 0.4083824008155395\n","Epoch 2/10, Loss: 0.23244302519871032\n","Epoch 3/10, Loss: 0.12542971398814937\n","Epoch 4/10, Loss: 0.05392120452362339\n","Epoch 5/10, Loss: 0.030079251433535854\n","Epoch 6/10, Loss: 0.019330463757101183\n","Epoch 7/10, Loss: 0.014871622828543289\n","Epoch 8/10, Loss: 0.013533182609501851\n","Epoch 9/10, Loss: 0.011851106654844176\n","Epoch 10/10, Loss: 0.010558614400826413\n","Accuracy:  0.8754539340954943\n","Precision:  0.8642315179295804\n","Recall:  0.8754539340954943\n","F1-score:  0.8670934153477852\n","Confusion Matrix:\n"," [[  96  280   53]\n"," [  93 5387  277]\n"," [  17  206 1026]]\n"]}]}]}