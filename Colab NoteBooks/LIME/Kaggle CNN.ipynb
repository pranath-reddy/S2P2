{"cells":[{"cell_type":"code","execution_count":null,"id":"pRMeb1RXi0Xw","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17624,"status":"ok","timestamp":1722181381700,"user":{"displayName":"Pranath Reddy Kumbam","userId":"13245202322626445782"},"user_tz":240},"id":"pRMeb1RXi0Xw","outputId":"0cb97ab3-c131-4078-f8e8-1a60250e9fff"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"id":"pRlBaQW9i0cA","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1722181381701,"user":{"displayName":"Pranath Reddy Kumbam","userId":"13245202322626445782"},"user_tz":240},"id":"pRlBaQW9i0cA","outputId":"09330022-d2db-46ef-ef24-06e5b29a0bf9"},"outputs":[],"source":["cd drive/My \\Drive/NLP"]},{"cell_type":"code","execution_count":null,"id":"tc1Fh_rKz96-","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":109179,"status":"ok","timestamp":1722181490878,"user":{"displayName":"Pranath Reddy Kumbam","userId":"13245202322626445782"},"user_tz":240},"id":"tc1Fh_rKz96-","outputId":"461ec16a-94b3-4f2c-ed13-0314e75d9991"},"outputs":[],"source":["!pip install textattack==0.3.7"]},{"cell_type":"code","execution_count":null,"id":"cesQh3Ds29H9","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7186,"status":"ok","timestamp":1722181498058,"user":{"displayName":"Pranath Reddy Kumbam","userId":"13245202322626445782"},"user_tz":240},"id":"cesQh3Ds29H9","outputId":"a89d3ea2-88db-424f-d556-3e8cbc1f5202"},"outputs":[],"source":["!pip install lime"]},{"cell_type":"code","execution_count":null,"id":"86980d01-4250-4b43-b306-6f7587b0edc8","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":29027,"status":"ok","timestamp":1722181527068,"user":{"displayName":"Pranath Reddy Kumbam","userId":"13245202322626445782"},"user_tz":240},"id":"86980d01-4250-4b43-b306-6f7587b0edc8","outputId":"b5dc1182-424c-412c-e64c-f2c11ff15444"},"outputs":[],"source":["# Load libraries\n","import pandas as pd\n","import numpy as np\n","import nltk\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader, TensorDataset\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import precision_recall_fscore_support, confusion_matrix\n","from lime.lime_text import LimeTextExplainer\n","import random\n","import transformers\n","import textattack\n","from textattack.datasets import Dataset\n","from datasets import load_dataset\n","from transformers import GPT2Tokenizer\n","from textattack.models.wrappers import ModelWrapper\n","from textattack.models.wrappers import HuggingFaceModelWrapper\n","from textattack import Attack\n","from textattack.transformations import WordSwapEmbedding\n","from textattack.search_methods import GreedyWordSwapWIR\n","from textattack.constraints.pre_transformation import RepeatModification, StopwordModification\n","from textattack.constraints.semantics import WordEmbeddingDistance\n","from lime.lime_text import LimeTextExplainer\n","from nltk.corpus import wordnet\n","\n","nltk.download('omw-1.4')\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","nltk.download('stopwords')"]},{"cell_type":"code","execution_count":null,"id":"avOpC22wzqtG","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":312363,"status":"ok","timestamp":1722181839428,"user":{"displayName":"Pranath Reddy Kumbam","userId":"13245202322626445782"},"user_tz":240},"id":"avOpC22wzqtG","outputId":"5980a9fd-69d8-445e-9762-59ba39b26770"},"outputs":[],"source":["# Load the dataset\n","df = pd.read_csv('./Data/KaggleData.csv')\n","\n","# Preprocess the tweets\n","df = df.dropna(subset=['tweet', 'class'])\n","df = df.reset_index(drop=True)\n","\n","# Convert to lowercase, remove punctuation, extra spaces, URLs, mentions, and hashtags\n","df['tweet'] = df['tweet'].str.lower().replace(r'[^\\w\\s]', '', regex=True).replace(' {2,}', ' ', regex=True).replace('\"', '')\n","df['tweet'] = df['tweet'].replace(r'http\\S+|www.\\S+|@\\w+|#\\w+', '', regex=True)\n","\n","# Download NLTK resources\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","nltk.download('stopwords')\n","\n","# Tokenization\n","df['tweet'] = df['tweet'].apply(nltk.word_tokenize)\n","\n","# Lemmatization\n","lemmatizer = nltk.WordNetLemmatizer()\n","df['tweet'] = df['tweet'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n","\n","# Removing stop-words\n","stop_words = set(nltk.corpus.stopwords.words('english'))\n","df['tweet'] = df['tweet'].apply(lambda x: [word for word in x if word not in stop_words])\n","\n","# Prepare data for DataLoader\n","max_length = 50\n","df['tweet'] = df['tweet'].apply(lambda x: x[:max_length])\n","\n","# Create word2index dictionary\n","word2index = {}\n","for tweet in df['tweet']:\n","    for word in tweet:\n","        if word not in word2index:\n","            word2index[word] = len(word2index)\n","\n","# Convert words to indices and pad sequences\n","input_data = np.zeros((len(df['tweet']), max_length), dtype=int)\n","for i, tweet in enumerate(df['tweet']):\n","    for j, word in enumerate(tweet):\n","        input_data[i, j] = word2index[word]\n","\n","# Encode labels\n","# 0 - hate speech, 1 - offensive language, 2 - neither\n","le = LabelEncoder()\n","y = le.fit_transform(df['class'])\n","\n","# Split the data\n","X_train, X_test, y_train, y_test = train_test_split(input_data, y, test_size=0.3, stratify=y, random_state=42)\n","\n","# Create DataLoader objects\n","batch_size = 64\n","train_dataset = TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))\n","test_dataset = TensorDataset(torch.from_numpy(X_test), torch.from_numpy(y_test))\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n","\n","# Create CNN model\n","class TextCNN(nn.Module):\n","    def __init__(self, vocab_size, embed_dim, num_classes):\n","        super(TextCNN, self).__init__()\n","        self.embedding = nn.Embedding(vocab_size, embed_dim)\n","        self.conv1 = nn.Conv2d(1, 100, (3, embed_dim))\n","        self.conv2 = nn.Conv2d(1, 100, (4, embed_dim))\n","        self.conv3 = nn.Conv2d(1, 100, (5, embed_dim))\n","        self.fc = nn.Linear(300, num_classes)\n","\n","    def forward(self, x):\n","        x = self.embedding(x).unsqueeze(1)\n","        x1 = F.relu(self.conv1(x)).squeeze(3)\n","        x2 = F.relu(self.conv2(x)).squeeze(3)\n","        x3 = F.relu(self.conv3(x)).squeeze(3)\n","        x1 = F.max_pool1d(x1, x1.size(2)).squeeze(2)\n","        x2 = F.max_pool1d(x2, x2.size(2)).squeeze(2)\n","        x3 = F.max_pool1d(x3, x3.size(2)).squeeze(2)\n","        x = torch.cat((x1, x2, x3), 1)\n","        x = self.fc(x)\n","        return x\n","\n","# Training\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","vocab_size = len(word2index)\n","embed_dim = 100\n","num_classes = len(np.unique(y))\n","model = TextCNN(vocab_size, embed_dim, num_classes).to(device)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","num_epochs = 10\n","for epoch in range(num_epochs):\n","    model.train()\n","    running_loss = 0.0\n","    for i, (inputs, labels) in enumerate(train_loader):\n","        inputs, labels = inputs.to(device), labels.to(device)\n","        optimizer.zero_grad()\n","        outputs = model(inputs.long())\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        running_loss += loss.item()\n","\n","    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {running_loss / len(train_loader)}\")\n","\n","    # Save Model\n","    torch.save(model, './Weights/KaggleCNN.pth')\n","\n","model = torch.load('./Weights/KaggleCNN.pth')\n","\n","# Test the model and collect predictions and true labels\n","model.eval()\n","predictions = []\n","true_labels = []\n","\n","with torch.no_grad():\n","    for inputs, labels in test_loader:\n","        inputs, labels = inputs.to(device), labels.to(device)\n","        outputs = model(inputs.long())\n","        _, predicted = torch.max(outputs, 1)\n","        predictions.extend(predicted.cpu().numpy())\n","        true_labels.extend(labels.cpu().numpy())\n","\n","# Calculate accuracy, precision, recall, F1-score, and confusion matrix\n","accuracy = np.mean(np.array(predictions) == np.array(true_labels))\n","precision, recall, f1_score, _ = precision_recall_fscore_support(true_labels, predictions, average='weighted')\n","conf_mat = confusion_matrix(true_labels, predictions)\n","\n","print(\"Accuracy: \", accuracy)\n","print(\"Precision: \", precision)\n","print(\"Recall: \", recall)\n","print(\"F1-score: \", f1_score)\n","print(\"Confusion Matrix:\\n\", conf_mat)"]},{"cell_type":"code","execution_count":null,"id":"hKt0Gow7-U80","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23,"status":"ok","timestamp":1722181839428,"user":{"displayName":"Pranath Reddy Kumbam","userId":"13245202322626445782"},"user_tz":240},"id":"hKt0Gow7-U80","outputId":"f36ee385-c8cc-425d-e530-df7eee2b0d6c"},"outputs":[],"source":["# Wrapper for the TextCNN model\n","class CNNBaselineWrapper:\n","    def __init__(self, model):\n","        self.model = model\n","\n","    def __call__(self, text_input_list):\n","        preds = []\n","        for text in text_input_list:\n","            input_tensor = tokenize_and_pad([text]).long()\n","            output = self.model(input_tensor.to(device))\n","            pred = torch.softmax(output, dim=1).squeeze().tolist()\n","            preds.append(pred)\n","        return np.array(preds)\n","\n","def tokenize_and_pad(text_list):\n","    max_length = 50\n","    tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n","    tokens = [tokenizer.tokenize(text)[:max_length] for text in text_list]\n","    token_indices = np.zeros((len(tokens), max_length), dtype=int)\n","    for i, tweet in enumerate(tokens):\n","        for j, word in enumerate(tweet):\n","            if word in word2index:\n","                token_indices[i, j] = word2index[word]\n","    return torch.tensor(token_indices)\n","\n","wrapped_model = CNNBaselineWrapper(model)\n","class_names = ['hate_speech', 'offensive_language', 'neither']\n","\n","# Explainability\n","def lime_analysis(text, wrapped_model, class_names):\n","    explainer = LimeTextExplainer(class_names=class_names)\n","    exp = explainer.explain_instance(text, wrapped_model, num_features=10, num_samples=2)\n","    return exp.as_list()\n","\n","df['tweet'] = df['tweet'].apply(lambda x: ' '.join(x))\n","text_to_explain = random.choice(df['tweet'])\n","print(\"Text to explain:\", text_to_explain)\n","lime_results = lime_analysis(text_to_explain, wrapped_model, class_names)\n","print(\"LIME analysis results:\")\n","print(lime_results)\n","\n","def calculate_doe(lime_results):\n","    feature_scores = [abs(score) for _, score in lime_results]\n","    std_dev = np.std(feature_scores)\n","    significant_features = len([score for score in feature_scores if score > std_dev])\n","    return significant_features / len(feature_scores)\n","\n","doe = calculate_doe(lime_results)\n","print(\"Degree of Explainability (DoE):\", doe)"]},{"cell_type":"code","execution_count":null,"id":"45PPQj_USaRs","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1722181839428,"user":{"displayName":"Pranath Reddy Kumbam","userId":"13245202322626445782"},"user_tz":240},"id":"45PPQj_USaRs","outputId":"de62bac5-e4a8-4644-fb8b-458fae60dec8"},"outputs":[],"source":["# Define number of samples for analysis\n","num_sam = 20\n","\n","# Load dataset\n","def load_custom_dataset(path):\n","    df = pd.read_csv(path)\n","    df = df.dropna(subset=['tweet', 'class'])\n","    return df\n","\n","# LIME Analysis\n","def lime_analysis(text, wrapped_model, class_names):\n","    explainer = LimeTextExplainer(class_names=class_names)\n","    exp = explainer.explain_instance(text, wrapped_model, num_features=10, num_samples=2)\n","    return exp.as_list()\n","\n","# Calculate Degree of Explainability (DoE)\n","def calculate_doe(lime_results):\n","    feature_scores = [abs(score) for _, score in lime_results]\n","    std_dev = np.std(feature_scores)\n","    significant_features = len([score for score in feature_scores if score > std_dev])\n","    return significant_features / len(feature_scores)\n","\n","# Calculate average DoE for Multiple samples\n","def calculate_average_doe(df, wrapped_model, class_names, samples=num_sam):\n","    doe_values = []\n","    sample_texts = random.sample(list(df['tweet']), samples)\n","    for text in sample_texts:\n","        lime_results = lime_analysis(text, wrapped_model, class_names)\n","        doe = calculate_doe(lime_results)\n","        doe_values.append(doe)\n","    average_doe = np.mean(doe_values)\n","    return average_doe\n","\n","# Path to the dataset\n","test_data_path = \"./Data/KaggleData.csv\"\n","df = load_custom_dataset(test_data_path)\n","\n","# Assuming wrapped_model and class_names are defined elsewhere\n","class_names = ['Hate speech', 'Offensive language', 'Neutral']\n","\n","# Calculate and print average DoE\n","average_doe = calculate_average_doe(df, wrapped_model, class_names)\n","print(f\"Average Degree of Explainability (DoE) for {num_sam} samples:\", average_doe)"]},{"cell_type":"code","execution_count":null,"id":"sdKbkuREwTCb","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12165,"status":"ok","timestamp":1722181851590,"user":{"displayName":"Pranath Reddy Kumbam","userId":"13245202322626445782"},"user_tz":240},"id":"sdKbkuREwTCb","outputId":"d283a998-25b7-4310-ee82-abe72c24637e"},"outputs":[],"source":["import numpy as np\n","from lime.lime_text import LimeTextExplainer\n","from nltk.corpus import wordnet\n","import random\n","import torch\n","import torch.nn.functional as F\n","\n","# Function to get synonyms for a word\n","def get_synonym(word):\n","    synonyms = set()\n","    for syn in wordnet.synsets(word):\n","        for lemma in syn.lemmas():\n","            if lemma.name() != word:\n","                synonyms.add(lemma.name().replace('_', ' '))\n","    return random.choice(list(synonyms)) if synonyms else word\n","\n","# Function to generate adversarial example using LIME\n","def generate_adversarial_example(text, predictor, explainer, num_features=2):\n","    exp = explainer.explain_instance(text, predictor, num_features=num_features)\n","    words = text.split()\n","    for feature, _ in exp.as_list()[:num_features]:\n","        if feature in words:\n","            idx = words.index(feature)\n","            words[idx] = get_synonym(words[idx])\n","    return ' '.join(words)\n","\n","# Wrapper for model prediction\n","def model_predict(texts):\n","    model.eval()\n","    indexed_texts = [[word2index.get(word, len(word2index)-1) for word in text.split()] for text in texts]\n","    padded_texts = torch.nn.utils.rnn.pad_sequence([torch.tensor(text) for text in indexed_texts], batch_first=True, padding_value=0)\n","    padded_texts = padded_texts.to(device)\n","    with torch.no_grad():\n","        outputs = model(padded_texts.long())\n","    return F.softmax(outputs, dim=1).cpu().numpy()\n","\n","# LIME-based adversarial attack\n","def lime_based_attack(dataset, samples=num_sam):\n","    correct_before_attack = 0\n","    correct_after_attack = 0\n","    total_samples = 0\n","\n","    explainer = LimeTextExplainer(class_names=['hate speech', 'offensive language', 'neither'])\n","\n","    for inputs, labels in dataset:\n","        for input_tensor, label in zip(inputs, labels):\n","            if total_samples >= samples:\n","                return total_samples, correct_before_attack, correct_after_attack\n","\n","            # Convert tensor to string\n","            text = ' '.join([list(word2index.keys())[i] for i in input_tensor.tolist() if i < len(word2index)])\n","\n","            # Original prediction\n","            original_pred = model_predict([text])[0].argmax()\n","            if original_pred == label:\n","                correct_before_attack += 1\n","\n","            # Generate adversarial example\n","            adv_text = generate_adversarial_example(text, model_predict, explainer)\n","            adv_pred = model_predict([adv_text])[0].argmax()\n","\n","            if adv_pred == label:\n","                correct_after_attack += 1\n","\n","            total_samples += 1\n","\n","    return total_samples, correct_before_attack, correct_after_attack\n","\n","# Perform the attack\n","total_samples, correct_before_attack, correct_after_attack = lime_based_attack(train_loader)\n","\n","# Calculate metrics\n","accuracy_before_attack = correct_before_attack / total_samples\n","accuracy_after_attack = correct_after_attack / total_samples\n","adv_rob = accuracy_after_attack / accuracy_before_attack if accuracy_before_attack > 0 else 0\n","\n","attack_resilience = adv_rob / average_doe if average_doe > 0 else 0\n","\n","# Print results\n","print(\"LIME-based adversarial attack results:\")\n","print(f\"Total samples: {total_samples}\")\n","print(f\"Correct before attack: {correct_before_attack}\")\n","print(f\"Correct after attack: {correct_after_attack}\")\n","print(f\"Accuracy before attack: {accuracy_before_attack}\")\n","print(f\"Accuracy after attack: {accuracy_after_attack}\")\n","print(\"\")\n","print(\"Results: \")\n","print(\"Adversarial Robustness (AdvRob):\", adv_rob)\n","print(\"Attack Resilience (Ar):\", attack_resilience)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.7"}},"nbformat":4,"nbformat_minor":5}
