{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2298,"status":"ok","timestamp":1680795592814,"user":{"displayName":"Pranath Reddy Kumbam","userId":"07194914569693395860"},"user_tz":240},"id":"uV8Lch-wnW32","outputId":"a3ccfc4a-0442-45a5-b2e1-4e6aeb6780d2"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1680795592815,"user":{"displayName":"Pranath Reddy Kumbam","userId":"07194914569693395860"},"user_tz":240},"id":"Fltk46feoNyW","outputId":"423d1164-a8a2-4997-b23f-91de24cba13f"},"outputs":[],"source":["cd drive"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1680795592815,"user":{"displayName":"Pranath Reddy Kumbam","userId":"07194914569693395860"},"user_tz":240},"id":"22rFozDqoQxm","outputId":"40289e1c-a021-401d-b1a0-1c3c89d30def"},"outputs":[],"source":["cd My \\Drive"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1680795592815,"user":{"displayName":"Pranath Reddy Kumbam","userId":"07194914569693395860"},"user_tz":240},"id":"DuSmOjVKoU-S","outputId":"5b12ba38-c4c5-41fe-dc82-e458a9564772"},"outputs":[],"source":["cd NLP/"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":131895,"status":"ok","timestamp":1680795724707,"user":{"displayName":"Pranath Reddy Kumbam","userId":"07194914569693395860"},"user_tz":240},"id":"ZMUG3iDxuEny","outputId":"732d0cea-1cf5-48b3-fdf0-9d3eefa6f086"},"outputs":[],"source":["# Load libraries\n","import pandas as pd\n","import nltk\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n","import numpy as np\n","from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import precision_recall_fscore_support\n","nltk.download('omw-1.4')\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","nltk.download('stopwords')\n","\n","# Load the dataset\n","df = pd.read_csv('./Data/KaggleData.csv')\n","\n","# Convert to lowercase, remove punctuation, extra spaces, URLs, mentions, and hashtags\n","df['tweet'] = df['tweet'].str.lower().replace(r'[^\\w\\s]', '', regex=True).replace(' {2,}', ' ', regex=True).replace('\"', '')\n","df['tweet'] = df['tweet'].replace(r'http\\S+|www.\\S+|@\\w+|#\\w+', '', regex=True)\n","\n","# Tokenization\n","nltk.download('punkt')\n","df['tweet'] = df['tweet'].apply(nltk.word_tokenize)\n","\n","# Lemmatization\n","nltk.download('wordnet')\n","lemmatizer = WordNetLemmatizer()\n","df['tweet'] = df['tweet'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n","\n","# Removing stop-words\n","nltk.download('stopwords')\n","stop_words = set(stopwords.words('english'))\n","df['tweet'] = df['tweet'].apply(lambda x: ' '.join([word for word in x if word not in stop_words]))\n","\n","# Create a custom dataset class\n","class TextDataset(Dataset):\n","    def __init__(self, texts, labels):\n","        self.texts = texts\n","        self.labels = labels\n","\n","    def __len__(self):\n","        return len(self.texts)\n","\n","    def __getitem__(self, idx):\n","        return self.texts[idx], self.labels[idx]\n","\n","# Encode the labels\n","# 0 - hate speech, 1 - offensive language, 2 - neither as positive or negative\n","label_encoder = LabelEncoder()\n","y = label_encoder.fit_transform(df['class'])\n","\n","# Splitting the Data using Stratified split\n","X_train, X_test, y_train, y_test = train_test_split(df['tweet'], y, test_size=0.3, stratify=y, random_state=42)\n","\n","# Tokenize and pad the input sequences\n","def tokenize_and_pad(texts, maxlen=100):\n","    tokenized_texts = [nltk.word_tokenize(text) for text in texts]\n","    return pad_sequence([torch.tensor([word_to_index[word] for word in text if word in word_to_index][:maxlen]) for text in tokenized_texts], batch_first=True, padding_value=len(word_to_index))\n","\n","word_to_index = {word: i for i, word in enumerate(set(df['tweet'].str.cat(sep=' ').split()), 1)}\n","X_train = tokenize_and_pad(X_train)\n","X_test = tokenize_and_pad(X_test)\n","\n","# Create PyTorch Datasets and DataLoaders\n","train_dataset = TextDataset(X_train, y_train)\n","test_dataset = TextDataset(X_test, y_test)\n","train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n","test_loader = DataLoader(test_dataset, batch_size=32)\n","\n","# Create a PyTorch LSTM model\n","class LSTMBaseline(nn.Module):\n","    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_classes):\n","        super().__init__()\n","        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n","        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n","        self.fc = nn.Linear(hidden_dim, num_classes)\n","\n","    def forward(self, x):\n","        x = self.embedding(x)\n","        packed_output, (hidden, cell) = self.lstm(x)\n","        x = self.fc(hidden[-1])\n","        return x\n","\n","# Initialize the model, optimizer, and loss function\n","model = LSTMBaseline(len(word_to_index) + 1, 50, 100, len(set(y)))\n","optimizer = optim.Adam(model.parameters(), lr=1e-3)\n","criterion = nn.CrossEntropyLoss()\n","    \n","# Train the model\n","epochs = 10\n","for epoch in range(epochs):\n","    model.train()\n","    epoch_loss = 0\n","\n","    for texts, labels in train_loader:\n","        optimizer.zero_grad()\n","        outputs = model(texts)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        epoch_loss += loss.item()\n","\n","    print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss / len(train_loader)}\")\n","\n","    # Save Model\n","    torch.save(model, './Weights/KaggleLSTM.pth')\n","\n","# Test the model and collect predictions and true labels\n","model.eval()\n","predictions = []\n","true_labels = []\n","\n","with torch.no_grad():\n","    for texts, labels in test_loader:\n","        outputs = model(texts)\n","        _, predicted = torch.max(outputs, 1)\n","        predictions.extend(predicted.numpy())\n","        true_labels.extend(labels.numpy())\n","\n","# Calculate accuracy, precision, recall, F1-score, and confusion matrix\n","accuracy = np.mean(np.array(predictions) == np.array(true_labels))\n","precision, recall, f1_score, _ = precision_recall_fscore_support(true_labels, predictions, average='weighted')\n","conf_mat = confusion_matrix(true_labels, predictions)\n","\n","print(\"Accuracy: \", accuracy)\n","print(\"Precision: \", precision)\n","print(\"Recall: \", recall)\n","print(\"F1-score: \", f1_score)\n","print(\"Confusion Matrix:\\n\", conf_mat)"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}
